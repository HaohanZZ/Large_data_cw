{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2: $\\ell_1$-Methods for Sparse Recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[] By tick the checkbox, we hereby declare that this coursework report is our own and autonomous work. We have acknowledged all material and sources used in its preparation, including books, articles, reports, lecture notes, internet software packages, and any other kind of document, electronic or personal communication. This work has not been submitted for any other assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Subgradient Method\n",
    "\n",
    "### 2.1.0 Differentiable Function\n",
    "\n",
    "Let $f(\\bm{x})$ be a differentiable function. We show that gradient descent direction leads a decrease of the objective function as follows.\n",
    "\n",
    "The first order Taylor approximation of $f(\\bm{x})$ is given by \n",
    "$$\n",
    "    f(\\bm{x} + \\tau \\bm{v}) \n",
    "    = f(\\bm{x}) \n",
    "    + \\langle \\nabla f(\\bm{x}), \\tau \\bm{v} \\rangle \n",
    "    + O(\\tau^2). \n",
    "$$\n",
    "Take the negative gradient direction, i.e., $\\bm{v} = -\\nabla f(\\bm{x})$. Then\n",
    "$$\n",
    "    f(\\bm{x} - \\tau \\nabla f(\\bm{x})) \n",
    "    = f(\\bm{x}) \n",
    "    - \\tau \\| \\nabla f(\\bm{x}) \\|^2_2\n",
    "    + O(\\tau^2). \n",
    "$$\n",
    "It is clearly when $\\tau>0$ is sufficiently small, \n",
    "$$\n",
    "    f(\\bm{x} -\\tau \\nabla f(\\bm{x})) \\le f(\\bm{x}),\n",
    "$$\n",
    "which proves our claim. \n",
    "\n",
    "We study the non-differentiable case below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Descent Direction? (25%)\n",
    "\n",
    "Let $\\bm{x} \\in \\mathbb{R}^2$ and \n",
    "$$\n",
    "    f(\\bm{x}) = 3 |x_1| +  |x_2|.\n",
    "$$\n",
    "Consider the point $\\bm{x}^0 = [0,1]^{\\mathsf{T}}$. \n",
    "1. Show that $\\bm{g} = [3,1]^{\\mathsf{T}} \\in \\partial f(\\bm{x})$. \n",
    "2. Let $\\tau \\in (0,1)$, find the closed form for \n",
    "$$\n",
    "      f(\\bm{x}^0 - \\tau \\bm{g} ).\n",
    "$$\n",
    "3. Comment on whether $-\\bm{g}$ is a descent direction or not.\n",
    "\n",
    "This exercise shows that sub-gradient linear search may not lead to a decrease of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Wolfe's Example (25%)\n",
    "\n",
    "Let $\\bm{x}\\in \\mathbb{R}^2$ and \n",
    "$$\n",
    "    f(\\bm{x}) = \\begin{cases}\n",
    "        5(9x_1^2 + 16 x_2^2)^{1/2} & \\text{if}~ x_1 > |x_2|, \\\\\n",
    "        9x_1 + 16|x_2| & \\text{if}~ x_1 \\le |x_2|.\n",
    "    \\end{cases}\n",
    "$$\n",
    "Suppose that $\\bm{x}^0 = [16/9,1]^{\\mathsf{T}}$. Consider exact line search where \n",
    "$$\n",
    "   \\bm{x}^{l+1} = \\bm{x}^l - t^{l+1} \\nabla f(\\bm{x}^l),\n",
    "$$\n",
    "where \n",
    "$$\n",
    "   t^{l+1} = \\arg~ \\min_t~ f(\\bm{x}^l - t \\nabla f(\\bm{x}^l)).\n",
    "$$\n",
    "1. Draw the contours of $f(\\bm{x})$ in the region $-2 \\le x_1 \\le 2$ and $-2 \\le x_2 \\le 2$. (The function `Plots > contour` may be useful.)\n",
    "2. Is the point $\\bm{x} = [0,0]^{\\mathsf{T}}$ the global optimal? Why?\n",
    "3. Find the closed form of $\\nabla f(\\bm{x})$ in the region where $x_1 > |x_2|$.\n",
    "4. Find $t^1$ and $\\bm{x}^1$ numerically. Note that in the region where $x_1 > |x_2|$\n",
    "   $$\n",
    "    \\left\\langle \n",
    "      \\nabla f \\left(\\bm{x}^l+1\\right), \\nabla f \\left(\\bm{x}^l\\right)\n",
    "    \\right\\rangle\n",
    "    = \\left\\langle \n",
    "      \\nabla f \\left( \\bm{x}^l - t \\nabla f \\left(\\bm{x}^l\\right) \\right), \\nabla f \\left(\\bm{x}^l\\right)\n",
    "    \\right\\rangle = 0. \n",
    "   $$\n",
    "5. Similarly, find $t^2$ and $\\bm{x}^2$, and $t^3$ and $\\bm{x}^3$ numerically. \n",
    "6. (Challenging) Use mathematical induction, show that $\\bm{x}^l \\rightarrow [0,0]^{\\mathsf{T}}$.  \n",
    "\n",
    "Wolfe's example shows that sub-gradient method may not converge to a global optimal point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sparse Recovery: Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISTA Implementation and Test (50%)\n",
    "\n",
    "Implement ISTA algorithm as a function `rec_ista` to solve the Lasso problem\n",
    "$$\n",
    "    \\frac{1}{2} \\| y- Ax \\|_2^2 + \\lambda \\| x \\|_1.\n",
    "$$\n",
    "where $\\lambda > 0$ is the regularization constant. \n",
    "\n",
    "Make sure that the regularization constant $\\lambda$ and the stepsize $\\tau$ are among input parameters.\n",
    "\n",
    "Suggested parameter for testing: $m=32,~ n=64,~ S=8$. Noise variance can be set small $\\sigma^2 = 1e-2 \\text{ or } 1e-4$.\n",
    "\n",
    "1. For a given regularization parameter $\\lambda > 0$, try different values of the step size $\\tau$, some small compared to $2/\\sigma_{\\max}(\\bm{A}^{\\mathsf{T}}\\bm{A})$ and some larger than it, for example, $20/\\sigma_{\\max}(\\bm{A}^{\\mathsf{T}}\\bm{A})$. Discuss the convergence of ISTA when $\\tau$ varies.\n",
    "2. With proper $\\tau$ so that ISTA converges, use numerical simulations to discuss how to choose an appropriate $\\lambda$ to get reasonable recovery $\\hat{x}$.\n",
    "3. Numerically compare ISTA and SP algorithms for noiseless sparse recovery problem ($\\sigma^2 = 0$). Discuss your observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supp_of_H (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "using SparseArrays\n",
    "using StatsBase\n",
    "using JLD2\n",
    "\n",
    "# Generate Gaussian Matrix\n",
    "function gen_GaussianMat(m,n)\n",
    "    #normalize columns\n",
    "        A = randn(m,n)\n",
    "        for i = 1:n\n",
    "            v = A[:,i]\n",
    "            A[:,i] = v/norm(v,2)\n",
    "        end\n",
    "        return A\n",
    "    end\n",
    "\n",
    "#Generate Sparse Matrix\n",
    "\n",
    "function sparse_data_gen(m::Int64,n::Int64,S::Int64,σ::Float64)\n",
    "\n",
    "    A = gen_GaussianMat(m,n) # generate a nomalized m by n matrix\n",
    "    items = [j for j= 1:n]\n",
    "    wv = [1 for j = 1:n]\n",
    "    w = σ*randn(Float64,m,1) # generate the noise with variance σ\n",
    "    # sparse support, or index.\n",
    "    x = zeros(Float64,n)\n",
    "    sparseSupport = sample( items , Weights(wv) , S, replace = false) # pick S random position for the non zero elements \n",
    "    randnTemp = randn(S, 1)                                           # of x\n",
    "    x[sparseSupport] = randnTemp #fill x with all the non-zero elements\n",
    "    \n",
    "    y = A*x + w\n",
    "\n",
    "    return y,A,x,sparseSupport \n",
    "end\n",
    "\n",
    "function supp_of_H(vector, s) # function that returns the indices of s values with the highest norm\n",
    "    H=[1] \n",
    "    if s > 1 \n",
    "        for i in 2:s \n",
    "            append!(H,i) \n",
    "        end \n",
    "    end \n",
    "    small_vector = vector[1:s] # we assume the first s values in the vector have the highest norm\n",
    "    for i in (s+1):length(vector) # starting from s+1 we check if there is another element with a higher norm\n",
    "        if abs(vector[i]) > minimum(abs.(small_vector)) \n",
    "            H[argmin(abs.(small_vector))] = i # replace the index \n",
    "            small_vector[argmin(abs.(small_vector))] = vector[i] # replace the value \n",
    "        end \n",
    "    end\n",
    "    return H \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec_ista (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rec_ista(x0,y,A,λ,τ,n,max_iter,stop_crit)\n",
    "\n",
    "    #Define proximity operator\n",
    "    #soft thresholding, y is a vector\n",
    "    prox_l1(y,γ) = max.(abs.(y) - γ*ones(size(y)),0).*sign.(y);  \n",
    "\n",
    "    f(x) = λ*norm(x,1) + (norm(y-A*x)).^2\n",
    "\n",
    "    #gradient_h\n",
    "    #this gradient_h is used for updating x\n",
    "    grad_h(A,y,x) = transpose(A)*(A*x - y)\n",
    "    \n",
    "    #initialization\n",
    "    x_cur = zeros(size(x0))\n",
    "    x_next = zeros(size(x0));\n",
    "    x_sol = zeros(size(x_cur))\n",
    "    iter = 0\n",
    "\n",
    "    ## main loop\n",
    "    while iter < max_iter\n",
    "\n",
    "        #gradient h\n",
    "        #a forward step, for updating x\n",
    "        r = x_cur - τ * grad_h(A,y,x_cur)\n",
    "        r = reshape(r,n)\n",
    "\n",
    "        #proximal operator of l1\n",
    "        x_next = prox_l1(r,λ*τ)\n",
    "        #calculate current f value\n",
    "        f_cur = f(x_cur)\n",
    "        #calculate next f value\n",
    "        f_next = f(x_next)\n",
    "        #calculate gap between f_cur and f_next\n",
    "        error = f_next - f_cur \n",
    "        \n",
    "        #break condition\n",
    "            if (norm((x_next-x_cur),2)<stop_crit)||(norm(error)/norm(f_cur)<stop_crit)\n",
    "                x_sol = x_next\n",
    "                return x_sol, error, iter\n",
    "                break\n",
    "            end\n",
    "\n",
    "        iter = iter + 1\n",
    "        x_cur = x_next\n",
    "    end   \n",
    "    \n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3523436612274402"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#signal generation\n",
    "m,n,S,σ = 32,64,8,1e-2\n",
    "y,A,x_origin,sparseSupport = sparse_data_gen(m,n,S,σ);\n",
    "\n",
    "x0 = zeros(size(x_origin))\n",
    "\n",
    "#  find τ bounds\n",
    "U,Sv,Vt = svd(transpose(A)*A)\n",
    "lamda_max = maximum(Sv)\n",
    "τ_upper = 2/(lamda_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort(Supp_x_rebuilt) = [6, 13, 18, 21, 23, 38, 46, 51]\n",
      "sort(sparseSupport) = [6, 13, 18, 21, 23, 38, 46, 51]\n",
      "x_rebuilt = [0.0, 0.0, -0.0, 0.0, 0.0, -0.9069821733015826, -0.0, 0.0, 0.00593313667147943, 0.0, 0.0, -0.0, 0.7145958851848304, 0.0216757475350265, 0.0, 0.0, -0.0, 1.063377542318859, -0.0, 0.0, -2.1182052740566335, -0.024202037171226855, -0.16927171113102957, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.007689128965483521, 0.0, 0.0, -0.007976431113765973, -0.009793580563681866, 0.0, -0.0, 0.7382733729234541, 0.00110835489283321, 0.0, 0.0, -0.011131492545257119, 0.026361753636271912, 0.0, 0.0, 0.5318120244540697, -0.0331355260463475, -0.0, -0.0, 0.0015400363610407549, -0.7951835604775653, -0.01459617667745898, -0.0, -0.0, 0.0, 0.0, 0.0, -0.021784359187994224, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0]\n",
      "x_origin = [0.0, 0.0, 0.0, 0.0, 0.0, -0.9078001422086724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7342134696719567, 0.0, 0.0, 0.0, 0.0, 1.1044865146850764, 0.0, 0.0, -2.15083952312961, 0.0, -0.2071747424357295, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8207326826328395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5735892145764856, 0.0, 0.0, 0.0, 0.0, -0.8064186578953685, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "norm(x_origin - x_rebuilt) = 0.13101778625819113\n",
      "iteration = 46515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46515"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#τ--the step size\n",
    "#τ = τ_upper/3\n",
    "τ = 0.01\n",
    "λ = 1e-2 \n",
    "\n",
    "#options\n",
    "max_iter = 100000\n",
    "stop_crit = 1e-7\n",
    "\n",
    "x_rebuilt, f_gap, iteration = rec_ista(x0,y,A,λ,τ,n,max_iter,stop_crit)\n",
    "\n",
    "Supp_x_rebuilt = supp_of_H(x_rebuilt,S) \n",
    "\n",
    "#original supportset and rebuilt supportset\n",
    "@show sort(Supp_x_rebuilt)\n",
    "@show sort(sparseSupport);\n",
    "\n",
    "#original signal and rebuilt signal\n",
    "@show x_rebuilt;\n",
    "@show x_origin;\n",
    "\n",
    "#error between two signals\n",
    "@show norm(x_origin - x_rebuilt);\n",
    "@show iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort(Supp_x_rebuilt) = [6, 13, 18, 21, 23, 38, 46, 51]\n",
      "sort(sparseSupport) = [6, 13, 18, 21, 23, 38, 46, 51]\n",
      "x_rebuilt = [0.0, 0.0, -0.0, 0.0, 0.0, -0.9069284031156662, -0.0, 0.0, 0.006020129485462563, 0.0, 0.0, -0.0, 0.7164997634174161, 0.021204731101228128, 0.0, 0.0, -0.0, 1.065998392790719, -0.0, 0.0, -2.1199950447571245, -0.022262079096950233, -0.1706604577843586, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.007202755687989714, 0.0, 0.0, -0.007863817256952857, -0.008599548076146126, 0.0, -0.0008339959262613126, 0.743525944546824, 0.0014209766532868626, 0.0, 0.0008045775336992674, -0.008210376670983146, 0.02453783606983662, 0.0, 0.0, 0.5326677695747216, -0.02956774488043089, -0.0, -0.0, 0.0009883584683034025, -0.7961953252248531, -0.013584806885121514, -0.0, -0.0, 0.0, 0.0, 0.0, -0.020866276071094845, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0]\n",
      "x_origin = [0.0, 0.0, 0.0, 0.0, 0.0, -0.9078001422086724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7342134696719567, 0.0, 0.0, 0.0, 0.0, 1.1044865146850764, 0.0, 0.0, -2.15083952312961, 0.0, -0.2071747424357295, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8207326826328395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5735892145764856, 0.0, 0.0, 0.0, 0.0, -0.8064186578953685, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "norm(x_origin - x_rebuilt) = 0.12311667699756526\n",
      "iteration = 1721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1721"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#τ--the step size\n",
    "τ = τ_upper-0.01\n",
    "λ = 1e-2 \n",
    "\n",
    "#options\n",
    "max_iter = 10000\n",
    "stop_crit = 1e-7\n",
    "\n",
    "x_rebuilt, f_gap, iteration = rec_ista(x0,y,A,λ,τ,n,max_iter,stop_crit)\n",
    "\n",
    "Supp_x_rebuilt = supp_of_H(x_rebuilt,S) \n",
    "\n",
    "#original supportset and rebuilt supportset\n",
    "@show sort(Supp_x_rebuilt)\n",
    "@show sort(sparseSupport);\n",
    "\n",
    "#original signal and rebuilt signal\n",
    "@show x_rebuilt;\n",
    "@show x_origin;\n",
    "\n",
    "#error between two signals\n",
    "@show norm(x_origin - x_rebuilt);\n",
    "@show iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function check_correct_bit(m,n,σ,total,max_iter,stop_crit,check_crit)\n",
    "    \n",
    "#     Sparsity = collect(4:2:20)\n",
    "#     correct_bit_l1 = zeros(length(Sparsity))\n",
    "#     #options\n",
    "    \n",
    "#         for j = 1:length(Sparsity)\n",
    "#             s_check = Sparsity[j]\n",
    "#             success_l1 = 0\n",
    "           \n",
    "#             for i = 1:total\n",
    "#                 #signal sparse_sig_generation\n",
    "#                 y_check,A_check,x_origin,sparseSupport = sparse_data_gen(m,n,s_check,σ)\n",
    "\n",
    "#                 #  find τ bounds\n",
    "#                 U,Sv,Vt = svd(transpose(A_check)*A_check)\n",
    "#                 lamda_max = maximum(Sv)\n",
    "#                 τ_upper = 2/(lamda_max)\n",
    "\n",
    "#                 #τ--the step size\n",
    "#                 #τ = τ_upper/3\n",
    "\n",
    "#                 τ_check = τ_upper/7\n",
    "\n",
    "#                 λ = 1e-2 \n",
    "\n",
    "\n",
    "#                 #two MM algorithms \n",
    "#                 x_sol_l1,~ = rec_ista(x_origin,y_check,A_check,λ,τ_check,n,max_iter,stop_crit);\n",
    "#                Supp_x_sol_l1 = supp_of_H(x_sol_l1,s_check);\n",
    "\n",
    "#                 if ((norm(x_origin-x_sol_l1,2) < check_crit))&&(sort(Supp_x_sol_l1)==sort(sparseSupport))\n",
    "#                     success_l1 += 1\n",
    "#                 end\n",
    "\n",
    "            \n",
    "#             end\n",
    "#             correct_bit_l1[j] = success_l1/total\n",
    "        \n",
    "#         end\n",
    "\n",
    "#         return correct_bit_l1\n",
    "#         # return correct_bit_l1\n",
    "# end\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m,n,σ = 32,64,1e-2 # characteristics of checked matrices \n",
    "# total = 50 # number of iterations of each  \n",
    "# #correct_bit_l1= check_correct_bit(m,n,σ,total)\n",
    "# max_iter,stop_crit,check_crit = 1000000, 1e-7,1e-2\n",
    "# cor_rate_l1= check_correct_bit(m,n,σ,total,max_iter,stop_crit,check_crit)\n",
    "# #     @save \"check_algorithms.jld2\" correct_bit_l0_in correct_bit_l0_without_in correct_bit_omp correct_bit_sp #overwrite the existing saved values \n",
    "# # else\n",
    "# #    @load \"check_algorithms.jld2\" correct_bit_l0_in correct_bit_l0_without_in correct_bit_omp correct_bit_sp  \n",
    "# # end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using  Plots\n",
    "# S = collect(4:2:20)\n",
    "# plot(S,cor_rate_l1,title = \"Comparison of algorithms\",xlabel = \"Sparsity S\",\n",
    "# ylabel = \"Avrg correct rate\",xlims = (0, 20),lab = \"Lasso_L1\")\n",
    "# # plot!(S,correct_bit_l0_without_in,lab = \"MM_L0_without_indicator\")\n",
    "# # plot!(S,correct_bit_omp,lab = \"omp\")\n",
    "# # plot!(S,correct_bit_sp,lab = \"SP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "choose_τ (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Choose τ\n",
    "function choose_τ(m,n,σ,S,length_τ, total,max_iteration,stop_crit)\n",
    "    \n",
    "    τ_list = zeros(length_τ)\n",
    "    avg_τ = zeros(length_τ)\n",
    "    \n",
    "    #find average value of τ\n",
    "    for i = 1:total\n",
    "\n",
    "        #initialization of norm_list\n",
    "        norm_list = zeros(size(avg_τ))\n",
    "\n",
    "        #signal sparse_sig_generation  \n",
    "        y_ch,A_ch,x_origin,sparseSupport = sparse_data_gen(m,n,S,σ)\n",
    "        x0 = zeros(size(x_origin))\n",
    "\n",
    "        #  find τ bounds\n",
    "        U,Sv,Vt = svd(transpose(A_ch)*A_ch)\n",
    "        lamda_max = maximum(Sv)\n",
    "        τ_upper = 2/(lamda_max)\n",
    "    \n",
    "        #find a suitable τ\n",
    "        for j = 1:length_τ\n",
    "\n",
    "            #τ--the step size\n",
    "            #ex: τ = τ_upper/3\n",
    "            τ_ch = τ_upper/(length_τ + 1 - j)\n",
    "            λ = 1e-2 \n",
    "            τ_list[j] += τ_ch\n",
    "            #l1 algorithms \n",
    "            x_sol_τ_ch,gap,iteration = rec_ista(x0,y_ch,A_ch,λ,τ_ch,n,max_iteration,stop_crit);\n",
    "            #Supp_x_sol_τ_ch = supp_of_H(x_sol_τ_ch,S);\n",
    "\n",
    "            norm_list[j] += norm(x_origin-x_sol_τ_ch,2)\n",
    "\n",
    "        end\n",
    "        \n",
    "        avg_τ += norm_list\n",
    "        \n",
    "    end\n",
    "    avg_τ = avg_τ/total\n",
    "    τ_list = τ_list/total\n",
    "    return avg_τ,τ_list\n",
    "        \n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.11527971337955868, 0.11519132840925775, 0.11507888896915949, 0.11498638992938787, 0.11489137896296092, 0.11478661603601123, 0.1146907481963178, 0.11459377859914856, 0.11439707989730474, 0.11431443109522582  …  0.11332048887856697, 0.11318922441187547, 0.11308312326133896, 0.11297048276909402, 0.11285521084900114, 0.11272461583661836, 0.11258937034013122, 0.11165704072368447, 0.11095941673509016, 0.11068500253948592], [0.01280619345797414, 0.013247786335835317, 0.013720921562115159, 0.014229103842193488, 0.01477637706689325, 0.015367432149568972, 0.01600774182246768, 0.016703730597357577, 0.01746299107905565, 0.018294562082820213  …  0.03841858037392243, 0.04268731152658048, 0.04802322546740303, 0.054883686248460635, 0.06403096728987072, 0.07683716074784486, 0.09604645093480606, 0.12806193457974144, 0.19209290186961211, 0.38418580373922423])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plots\n",
    "using Plots\n",
    "m,n,S,σ = 32,64,8,1e-2 # characteristics of checked matrices\n",
    "max_iter,stop_crit = 100000, 1e-7\n",
    "length_τ = 30\n",
    "#total = 5\n",
    "total = 50\n",
    "avg_norm_list,τ_list = choose_τ(m,n,σ,S,length_τ,total,max_iter,stop_crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(τ_list,avg_norm_list,label = [\"avg norm between x_origin and x_rebuilt\"], lw = 2)\n",
    "\n",
    "# #total = 10\n",
    "# total = 10\n",
    "# avg_norm_list,τ_list = choose_τ(m,n,σ,S,length_τ,total,max_iter,stop_crit)\n",
    "# plot!(τ_list,avg_norm_list,label = [\"10 iterations\"], lw = 2)\n",
    "\n",
    "# #total = 15\n",
    "# total = 15\n",
    "# avg_norm_list,τ_list = choose_τ(m,n,σ,S,length_τ,total,max_iter,stop_crit)\n",
    "# plot!(τ_list,avg_norm_list,label = [\"15 iterations\"], lw = 2)\n",
    "\n",
    "# #total = 20\n",
    "# total = 20\n",
    "# avg_norm_list,τ_list = choose_τ(m,n,σ,S,length_τ,total,max_iter,stop_crit)\n",
    "# plot!(τ_list,avg_norm_list,label = [\"20 iterations\"], lw = 2)\n",
    "\n",
    "xlabel!(\"τ\")\n",
    "ylabel!(\" norm(x_origin-x_rebuilt,2) \")\n",
    "title!(\"τ VS norm\")\n",
    "savefig(\"τ VS norm\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision between SP and ISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subspace_pursuit (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SP implementation\n",
    "using Distributions\n",
    "using Random\n",
    "using JLD2\n",
    "\n",
    "function subspace_pursuit(y,A,s)\n",
    "    T = supp_of_H((A')*y,s) # take an initial T set of indices\n",
    "    T = sort(T) \n",
    "    A_T = A[:,T] # generate the sparse matrix A\n",
    "    y_res = y - A_T*pinv(A_T'*A_T)*A_T'*y # calculate the initial y residue\n",
    "    iterations = 0\n",
    "    xl = []\n",
    "    while true\n",
    "        iterations += 1\n",
    "        y_res_last = y_res # store the previous y residue\n",
    "        T_new = supp_of_H((A')*y_res,s) # get another set of s\n",
    "        T_union = union(T,T_new) # combine them to get a set of 2s (expand support)\n",
    "        T_union = sort(T_union)\n",
    "        A_T = A[:,T_union] # generate a 2s sparse matrix A\n",
    "        b = pinv(A_T'*A_T)*A_T'*y # estimate a 2s sparse signal\n",
    "        T = supp_of_H(b,s) # shrink support\n",
    "        T = T[1:s]\n",
    "        T = T_union[T]\n",
    "        A_T = A[:,sort(T)]\n",
    "        xl = pinv(A_T'*A_T)*A_T'*y # estimate s sparse signal\n",
    "        y_res = y - A_T*xl # compute estimation error\n",
    "        if ((abs((norm(y_res_last,2)-norm(y_res,2)))<1e-100)||(norm(y_res,2)==0 || iterations > 100))\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    x_final=zeros(1,length(A[1,:]))\n",
    "    x_final[1,sort(T)]=xl \n",
    "    return (x_final')\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: s not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: s not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[13]:1",
      " [2] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "source": [
    "subspace_pursuit(y,A,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlight\n",
    "\n",
    "Please list a couple of highlights of your coursework that may impress your markers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
